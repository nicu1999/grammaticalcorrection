# Romanian Grammatical  Error Correction
This project is my bachelor's thesis for my final year. It tries to tackle the natural language processing task of grammatical error correction for the Romanian language, using the pre-trained language model XLM-RoBERTa and fine-tuning it (on a token tagging task) to detect errors. Additionally, the pre-trained masked language head is employed to generate alternative suggestions, from which the most likely one is selected. Utilizing the same token tagging technique, an additional model has been trained to indicate the precise positions for the insertion of text of varying lengths. Said text is then generated by the mT5 language model, which was fine-tuned for text unmasking in romanian, similarly to BERT.

The error detection modules have both been trained on a corpus of 10 million incorrect/corrected sentence pairs that have been generated synthetically and on 100.000 sentence pairs generated by the GPT3.5 large language model. This is done by asking it to corrupt grammatically correct text. The whole configuration has been tested on the RONACC corpus made available in “Neural grammatical error correction for romanian”. We have not reached state of the art for Romanian grammatical error correction but we have seen improved results with the GPT3.5 generated corpus compared to the 10M synthetically generated corpus (it’s important to note that the GPT3.5 models took significantly less resources to train, by a factor of approximately 100).

## Context
The process of pinpointing and rectifying grammatical issues within an erroneous statement is referred to as grammatical error correction, commonly abbreviated as GEC. All types of grammatical errors, including misspellings, improper use of articles, prepositions, pronouns, nouns, etc., as well as weak sentence structure, can be included in this list. Grammar checkers are used in a variety of applications: email, programs, text editors, messages, and others. One key point is that many downstream tasks are dependent on the grammaticality of the input, making this task especially important.

GEC encounters distinct challenges in both extensive data and scarce data scenarios. Coping with large datasets in the big data context necessitates robust infrastructure, data quality assurance, and extended model training time. Conversely, low data scenarios present limited training examples, data sparsity, and a heightened risk of overfitting.

## Objective
This project tries to develop a robust GEC system, that is modular in nature, can adapt to low-resource languages (Romanian in this case), and provides a balance between full correction and granularity. The system should be capable of handling error from various sources, including but not limited to: lack of knowledge, inattentiveness, or non-native language use. The results have been evaluated against a standard corpus, as introduced in the previous paper, to benchmark the performance of our system.

## Solution

Our solutions primarily leverage the token annotation task to identify which tokens should be deleted or modified, and where to insert tokens. The goal, through the use of these operations, is to transform the incorrect sentence to the corrected variant. For this our system builds upon previous work. It uses both data generated using text corruption that was made available by COTET Et al. [1] and data generated using the GPT-3.5 model [3] and prompt engineering. We have named the first dataset 10M (after the total number of pairs, 10 millions) and the second one 100k (100.000 pairs).
We use a tokenizer and 4 models: the XLM-roBERTa tokenizer for pre-processing inputs, XLM-roBERTa-large for the Mask-Fill task, two XLM-XLM-roBERTa-base models using the Token Tagging head [4], where the first is finetuned to detect if a certain token should be deleted or modified (called from now on the MOD/DEL model) and the other if at a certain position there should be inserted a varied length sequence (called the ADD model). The last is a mT5 model fine-tuned for generating sequences of varied length.
We preprocess the data by taking pairs of sentences (incorrect and corrected) tokenizing them and passing them through a modified Wagner–Fischer algorithm to detect the operation necessary to transform the incorrect sentence to the corrected variant. Using this intermediary form we create 5 datasets: the MOD/DEL dataset (10M and 100k), the ADD dataset(10M and 100k) and the mT5 unmask dataset(10M).
For a grammatically incorrect sentence, we begin by tokenizing it using the XLM-RoBERTa tokenizer. Next, the token sequence is simultaneously inputted into both the MOD/DEL model and, if enabled, the ADD model.
Using the tags generated by the MOD/DEL model we delete the tokens that are marked for deletion and we replace the tokens that are marked for replacement with a <mask> token. We then input the new token sequence into the mask fill model, and get the topK replacements. In this implementation the most likely token is always used. If the ADD model is activated, it results in determining the specific location within the sequence where a variable-length string needs to be inserted. This string is subsequently generated by the fine-tuned mT5 model.

## Screenshots
![1st screenshot](./screenshots/Screenshot%202023-09-11%20at%2006.18.13.png)
![2nd screenshot](./screenshots/Screenshot%202023-09-11%20at%2006.22.04.png)
![3rd screenshot](./screenshots/Screenshot%202023-09-11%20at%2006.30.44.png)
## Used resources
The project was done with the help of: jupyter notebooks, the transformers library, the datasets library, the https://github.com/teodor-cotet/RoGEC dataset, cross language RoBERTa, numpy torch, OpenAI api, flask and React (for previewing the project).
## How it works
There are 3 parts to this project: token tagging (TokenTagging.ipynb), training the model (TagTrain.ipynb) and wraping it all up (GEC_final.ipynb).
## Dataset Generation
![Dataset generation](./screenshots/Dataset%20Generatio.png)
## Model Overview
![Model Overview](./screenshots/Model%20Overview.png) 
### Token tagging 
In this phase the goal is to make mask of how to get from the initial, wrong phrase, to the final correct one. An example:
> Wrong: Acoperișul din șindrilă nu se mai păstrează, fiind înlocuită cu țiglă în 1936, fapt ce a necesitat sprijinirea acoperișului cu structuri inprovizate.

> Correct: Acoperișul din șindrilă nu se mai păstrează, fiind înlocuită cu țiglă în 1936, fapt ce a necesitat sprijinirea acoperișului cu structuri improvizate.

The result of running the above example trough TokenTagging.ipynb is the following json object: 

>{"input_ids": [0, 62, 587, 17152, 9511, 202, 321, 828, 29887, 25846, 315, 40, 409, 30650, 9297, 5783, 4, 13374, 200002, 2622, 314, 6, 1878, 177, 25846, 346, 44500, 4, 27444, 405, 10, 34255, 18, 97823, 20884, 84603, 9511, 941, 314, 206199, 23, 70628, 77632, 5, 2], "attention_mask": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "labels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 0, 0], "add_mask": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}

The "input_ids" represents the ids of the tokens (we are using the roBERTa tokenizer), the "labels" represent the tokens that are marked for change (1) or deletion (2). I have used a modified dynamic programming levenshtain algorithm to find de differences betweem the wrong version of the sentance and the correct one.

### Training the Model

The base model used is the xlm-roberta-base (cross language model and fine tune it for token classification. The classification is, of course, whether to delete a token or to modify it.

### Wraping it all up

One of the task roBERTa is trained on is token masking, meaning that a token is removed, and the model is tasked with guessing which token was deleted. The model is using this to its advantage. The basic idea is that a finetuned roBERTa on toke classification highlights the tokens that should be deleted (trivial) and those that should be replaced (using an out of the box roBERTa model used for token masking).
