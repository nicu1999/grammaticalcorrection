# Romanian Grammatical  Error Correction
This project is my bachelor's thesis for my final year. It tries to tackle the natural language processing task of grammatical error correction for the Romanian language, using the pre-trained language model XLM-RoBERTa and fine-tuning it (on a token tagging task) to detect errors. Additionally, the pre-trained masked language head is employed to generate alternative suggestions, from which the most likely one is selected. Utilizing the same token tagging technique, an additional model has been trained to indicate the precise positions for the insertion of text of varying lengths. Said text is then generated by the mT5 language model, which was fine-tuned for text unmasking in romanian, similarly to BERT.

The error detection modules have both been trained on a corpus of 10 million incorrect/corrected sentence pairs that have been generated synthetically and on 100.000 sentence pairs generated by the GPT3.5 large language model. This is done by asking it to corrupt grammatically correct text. The whole configuration has been tested on the RONACC corpus made available in “Neural grammatical error correction for romanian”.

We have not reached state of the art for Romanian grammatical error correction but we have seen improved results with the GPT3.5 generated corpus compared to the 10M synthetically generated corpus (it’s important to note that the GPT3.5 models took significantly less resources to train, by a factor of approximately 100).

## Context
The process of pinpointing and rectifying grammatical issues within an erroneous statement is referred to as grammatical error correction, commonly abbreviated as GEC. All types of grammatical errors, including misspellings, improper use of articles, prepositions, pronouns, nouns, etc., as well as weak sentence structure, can be included in this list. Grammar checkers are used in a variety of applications: email, programs, text editors, messages, and others. One key point is that many downstream tasks are dependent on the grammaticality of the input, making this task especially important.

GEC encounters distinct challenges in both extensive data and scarce data scenarios. Coping with large datasets in the big data context necessitates robust infrastructure, data quality assurance, and extended model training time. Conversely, low data scenarios present limited training examples, data sparsity, and a heightened risk of overfitting.

## Objective
This project tries to develop a robust GEC system, that is modular in nature, can adapt to low-resource languages (Romanian in this case), and provides a balance between full correction and granularity. The system should be capable of handling error from various sources, including but not limited to: lack of knowledge, inattentiveness, or non-native language use. The results have been evaluated against a standard corpus, as introduced in the previous paper, to benchmark the performance of our system.

## Solution

Our solutions primarily leverage the token annotation task to identify which tokens should be deleted or modified, and where to insert tokens. The goal, through the use of these operations, is to transform the incorrect sentence to the corrected variant. For this our system builds upon previous work. It uses both data generated using text corruption that was made available by https://github.com/teodor-cotet/RoGEC and data generated using the GPT-3.5 model and prompt engineering. We have named the first dataset 10M (after the total number of pairs, 10 millions) and the second one 100k (100.000 pairs).

We use a tokenizer and 4 models: the XLM-roBERTa tokenizer for pre-processing inputs, XLM-roBERTa-large for the Mask-Fill task, two XLM-XLM-roBERTa-base models using the Token Tagging head [4], where the first is finetuned to detect if a certain token should be deleted or modified (called from now on the MOD/DEL model) and the other if at a certain position there should be inserted a varied length sequence (called the ADD model). The last is a mT5 model fine-tuned for generating sequences of varied length.

We preprocess the data by taking pairs of sentences (incorrect and corrected) tokenizing them and passing them through a modified Wagner–Fischer algorithm to detect the operation necessary to transform the incorrect sentence to the corrected variant. Using this intermediary form we create 5 datasets: the MOD/DEL dataset (10M and 100k), the ADD dataset(10M and 100k) and the mT5 unmask dataset(10M).

For a grammatically incorrect sentence, we begin by tokenizing it using the XLM-RoBERTa tokenizer. Next, the token sequence is simultaneously inputted into both the MOD/DEL model and, if enabled, the ADD model.
Using the tags generated by the MOD/DEL model we delete the tokens that are marked for deletion and we replace the tokens that are marked for replacement with a <mask> token. We then input the new token sequence into the mask fill model, and get the topK replacements. In this implementation the most likely token is always used. If the ADD model is activated, it results in determining the specific location within the sequence where a variable-length string needs to be inserted. This string is subsequently generated by the fine-tuned mT5 model.

## Screenshots
![1st screenshot](./screenshots/Screenshot%202023-09-11%20at%2006.18.13.png)
![2nd screenshot](./screenshots/Screenshot%202023-09-11%20at%2006.22.04.png)
![3rd screenshot](./screenshots/Screenshot%202023-09-11%20at%2006.30.44.png)

## Dataset Generation
![Dataset generation](./screenshots/Dataset%20Generatio.png)

We created five training datasets for our models using 3 distinct formats(MOD/DEL, ADD and mT5). Four of these datasets are designed for token tagging—two were trained on the base 10M dataset, and two were trained on the 100k dataset. Additionally, we have one seq2seq dataset, which was built using the 10M base dataset.

The base for all of our datasets is a pair of sentences: one that is incorrectly formed and its corresponding corrected version. For the following section we will use the pair

("Salut faci tu? Cum o mai druci", "Salut ce faci? Cum o mai duci")
named from now on (incorrect, correct) as examples for all of our data generation
procedures.

Next, we tokenize both sentences using the XLM-RoBERTa tokenizer, which includes both a start and an end token. We get the following list of tokens:

> incorrect: [0, 48721, 405, 10965, 32, 7140, 36, 409, 115, 318, 2]

> correct: [0, 48721, 10965, 370, 32, 7140, 36, 409, 26223, 318, 2]

These tokens map to the initial sentences in the following way:
> 0:\<s> 48721:Salut 10965:faci 370:tu 32:? 7140:Cum 36:o 409:mai 26223:dru 318:ci 2:\</s> 0:\<s> 48721:Salut 405:ce 10965:faci 32:? 7140:Cum 36:o 409:mai 115:du 318:ci 2:\</s>

The start and end tokens ( 0=\<s> and 2=\</s>) are vital, particularly when adding text to the sentence's end. Without the end token, adding text would be impossible.

We use the modified Wagner–Fischer algorithm algorithm to generate a list of operations that transforms the incorrect sentence to the corrected one. The main addition over the standard Wagner–Fischer algorithm is that each cell in the matrix contains not only the count of operations required to translate a specific prefix to another but also the previous prefix matrix coordinates from which it was derived.

We start from the final cell of the matrix, at M[n, m] and we iteratively compare the current prefix with the one from which it was formed from, and from their difference we can infer what operation was applied.
This leads to a list of operations that, when applied to the source sentence, yield the target sentence.

> [0, 0, 3, 0, 2, 0, 0, 0, 0, 1, 0, 0] where 0 is the “do nothing operation”, 1 is “modify”, 2 is “delete” and 3 is “add”.

This list is the base for all of our datasets.
Trimming the add operations, and packaging it alongside the tokenization incorrect sentence, yields the MOD/DEL dataset. The final form of our above example for the MOD/DEL dataset is:

>{
>“input_ids”: [0, 48721, 405, 10965, 32, 7140, 36, 409, 115, 318, 2], “attention_mask”: [1, 1, 1, 1 , 1, 1, 1, 1, 1, 1, 1],
>“labels”: [0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0]
>}

If we replace the mod and del operations with 0, and map all consecutive add operations to a single operation and couple this with the tokenization incorrect sentence we have the ADD dataset. Similarly to above we have:

>{
>“input_ids”: [0, 48721, 405, 10965, 32, 7140, 36, 409, 115, 318, 2], “attention_mask”: [1, 1, 1, 1 , 1, 1, 1, 1, 1, 1, 1],
>“labels”: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
>}

The generation process for the mT5 dataset follows a different approach. Given the inherent nature of mT5 as a seq2seq model, it necessitates both of the input and output to be string or arbitrary size. The input will be a clean sentence with some parts of it masked out, and the output are the masked tokens. The goal of the system is, for a given input masked sentence, to generate the most plausible missing tokens.
Initially, we identify and extract tokens marked with "add" from the operation list in the target sentence. Subsequently, we employ the mT5 mask ("<extra_id_{n}>," where 'n' varies between 0 and 99) to mask the positions from which these tokens were extracted. The output will be the extracted tokens, formatted like this: <extra_id_0> first group of tokens to generate <extra_id_1> second group of tokens to generate <extra_id_2>...
For our above example the resulting mT5 dataset would be: 
>{
>"input":"Salut<extra_id_0> faci? Cum o mai duci",
>"output":"<extra_id_0> ce<extra_id_1>" }

## Training

We have trained the models using the Hugging Face framework. For all of the XLM-R models we have used the “base” version, to maintain a balance between performance and training costs. We used the following hyperparameters as our training arguments:

>{
>evaluation_strategy = "epoch",
>learning_rate=2e-5,
>per_device_train_batch_size=64,
>per_device_eval_batch_size=64,
>num_train_epochs=2,
>weight_decay=0.01,
>save_total_limit = 1,
>save_strategy="epoch" }

The number of epochs has been limited to 2-3 due to performance constraints and to reduce overfitting on the training data.

## Model Overview
![Model Overview](./screenshots/Model%20Overview.png) 

Leveraging the fine-tuned models from the aforementioned generated corpora, this is how our grammatical error correction system is constructed.

The initial step involves tokenizing the grammatically incorrect sentence. It is imperative to note that both our MOD/DEL and ADD models exclusively accept input in the form of token lists. Following tokenization, the resultant tokens are subsequently processed through the MOD/DEL model. This results in a token mask, where on the 0 tag a token is correct, on the 1 tag it should be modified, and on the 2 tag it should be deleted.

The process proceeds as follows: Initially, tokens marked with "2" undergo deletion, accompanied by the concurrent updating of all associated masks, including both the MOD/DEL and ADD tags specific to the affected index.

Subsequently, tokens designated with "1" are replaced with the "<mask>" token and presented to the XLM-R large model, from which the top alternative is chosen as the token replacement.

If the ADD model is inactive, the final tokens are trimmed (removing start and end tokens) and refined, resulting in the corrected sentence.

However, when the ADD model is active, tokens are strategically inserted into this model, producing a tag mask. A tag located at a specific token's position indicates that immediately preceding that token, the mT5 mask should be inserted.

To accomplish this, we utilize tokens from the preceding steps, introducing a custom token (e.g., "<extra_id_0>"), which is then processed through the mT5 model to generate the required sentence components. These generated strings are seamlessly integrated into the original sentence, resulting in the corrected version.

## Testing And Results


We categorized our models in two distinct manners. The initial categorization depends on whether the model has the ADD module activated, which also implies the presence of the mT5 module. The second categorization is based on whether the MOD/DEL and ADD models were trained on the 10M corpus or the 100K corpus. Consequently, we have four distinct final models: the 10M-base, 10M-add, 100K-base, and 100K-add models.

Our rationale for this was that the addition of the ADD module could improve performance in some cases, but also be detrimental to it (as it could easily generate plausible yet irrelevant alternatives).

We evaluated our dataset using the Romanian version of ERRANT (foot note), which generated the required edits to transform the original incorrect sentences into their corrected counterparts. These are referred to as "original-golden" for the accurate, base edits, and "original-predicted" for the model's predicted revisions. By comparing the original-golden and the original-predicted edit lists, we can calculate the percentages of True Positives, False Positives, and False Negatives, which enables us to calculate the F0.5 score.

In our evaluation, a True Positive is identified when an edit appears in both the original-golden and original-predicted edit lists. Conversely, a False Positive occurs when an edit is found in the original-predicted list but not in the original-golden edit list. Similarly, a False Negative arises when an edit is absent from the original-predicted list but is present in the original-golden edit list.

We have tested on the RONACC corpus presented in COTET Et Al., and more specifically on the well formed written sentences partition (W-sentence). These are our result, which do not reach the state of the art presented in COTET Et al.:

| F0.5 10M-base | F0.5 10M-add | F0.5 100k-base | F0.5 100k-add |
|---------------|--------------|----------------|---------------|
| 35.4  | 36.5  | 35.1  | 38.1 |
