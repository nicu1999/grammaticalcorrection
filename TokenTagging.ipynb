{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":3953,"status":"ok","timestamp":1661808140065,"user":{"displayName":"Nicolae Rosca","userId":"09067692646217768074"},"user_tz":-180},"id":"Zpk9LnRS-YF_"},"outputs":[],"source":["%%capture\n","pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Used for Testing and prototyping"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":4946,"status":"ok","timestamp":1661808145006,"user":{"displayName":"Nicolae Rosca","userId":"09067692646217768074"},"user_tz":-180},"id":"z517dek98YEi"},"outputs":[],"source":["import pprint\n","from transformers import  AutoTokenizer\n","pp = pprint.PrettyPrinter(indent=4)\n","\n","tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base', model_max_length=512)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1661808145007,"user":{"displayName":"Nicolae Rosca","userId":"09067692646217768074"},"user_tz":-180},"id":"OMLkObvi3eIO","outputId":"53acebff-0dc1-4d63-ade7-a75d2be4b67b"},"outputs":[{"name":"stdout","output_type":"stream","text":["[   'Alte mărci comerciale utilizate vreme îndelungată sunt Löwenbräu, '\n","    'deținătorii căreia spun că este folosită din 1383, și Stella Artois  din '\n","    '1366.',\n","    'Alte mărci comerciale utilizate vreme îndelungată sunt Löwenbräu, '\n","    'deținători căreia spun că este folosită din 1383, și Stella Artois  din '\n","    '1366.',\n","    'Cu aceeași grafie a fost săpată și inscripția, tocită însă de vreme, pe '\n","    'prestolul de piatră cu profile.',\n","    'Cu aceași grafie a fost săpata și inscripția, tocită însă de vreme, pe '\n","    'prestolul de piatră cu profile.']\n"]}],"source":["import itertools\n","\n","def read_file(filename, top_k):\n","  doc = []\n","  with open(filename) as fileobject:\n","    top_2k = itertools.islice(fileobject, 2*top_k) \n","    for line in top_2k:\n","          doc.append(line[:-1])\n","  return doc\n","\n","doc = read_file('drive/MyDrive/Colab Notebooks/corpus/dev.txt', 2)\n","\n","pp.pprint(doc)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1661808145007,"user":{"displayName":"Nicolae Rosca","userId":"09067692646217768074"},"user_tz":-180},"id":"QSWmLBFS9JdP"},"outputs":[],"source":["def roberta_tokenize(doc):\n","  new_doc = []\n","  for i in range(0, len(doc), 2):\n","    new_doc.append((tokenizer.tokenize(doc[i]), tokenizer.tokenize(doc[i + 1])))  \n","  return new_doc\n","\n","\n","def print_pair(doc):\n","  for correct, wrong in doc:\n","    print(correct)\n","    print(wrong)\n","    print(\"\")\n","\n","\n","token_doc = roberta_tokenize(doc)"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1661808145008,"user":{"displayName":"Nicolae Rosca","userId":"09067692646217768074"},"user_tz":-180},"id":"Pgx4TAhaWaKT"},"outputs":[],"source":["def levenshtein(tokens1, tokens2):\n","  if len(tokens1) == 0: #len(t) inserari\n","    return len(tokens2)\n","\n","  if len(tokens2) == 0: #len(t) inserari\n","    return len(tokens1)\n","\n","  if tokens1[0] == tokens2[0]: #0 operatii\n","     return levenshtein(tokens1[1:], tokens2[1:])\n","  \n","  res1 = levenshtein(tokens1[1:], tokens2) #primul din token2 e sters\n","\n","  res2 = levenshtein(tokens1, tokens2[1:]) #primul din token2 e adaugat\n","\n","  res3 = levenshtein(tokens1[1:], tokens2[1:]) #primele sunt diferite\n","\n","  return 1 + min(res1, res2, res3)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1661808145008,"user":{"displayName":"Nicolae Rosca","userId":"09067692646217768074"},"user_tz":-180},"id":"TEWsRyHRKX1s","outputId":"9abd4a36-2e39-4d5a-caca-29e3421ccac1"},"outputs":[{"name":"stdout","output_type":"stream","text":["['▁câștig', 'uri']\n","['▁că', 'ști', 'guri']\n"]}],"source":["def add_opp(lst, opp, n):\n","  for i in range(n):\n","    lst.append(opp)\n","  return lst\n","\n","def best_distance(res1, res2, res3):\n","  res1_d = len(res1)\n","  res2_d = len(res2)\n","  res3_d = len(res3)\n","\n","  min_d = res1_d\n","  min = res1\n","\n","  if res2_d < min_d:\n","     min_d = res2_d\n","     min = res2\n","\n","  if res3_d < min_d:\n","     min_d = res3_d\n","     min = res3\n","    \n","  return min\n","\n","def pseudo_levenshtein(tokens1, tokens2, ops):\n","  if len(tokens1) == 0: #len(t) inserari\n","    return add_opp(ops, 'add', len(tokens2))\n","\n","  if len(tokens2) == 0: #len(t) inserari\n","    return add_opp(ops, 'del', len(tokens1))\n","\n","  if tokens1[0] == tokens2[0]: #0 operatii\n","    return pseudo_levenshtein(tokens1[1:], tokens2[1:], ops)\n","\n","  ops1 = ops.copy()\n","  ops1.append(\"del\")\n","  ops2 = ops.copy()\n","  ops2.append(\"add\")\n","  ops3 = ops.copy()\n","  ops3.append(\"dif\")\n","\n","  res1 = pseudo_levenshtein(tokens1[1:], tokens2, ops1) #primul din token2 e sters\n","  res2 = pseudo_levenshtein(tokens1, tokens2[1:], ops2) #primul din token2 e adaugat\n","  res3 = pseudo_levenshtein(tokens1[1:], tokens2[1:], ops3) #primele sunt diferite\n","  min = best_distance(res1, res2, res3)\n","\n","  return min\n","\n","\n","pair1 = [\"câștiguri\", \"căștiguri\"]\n","pair2 = [\"șeful departamentului politic\", \n","         \"șef Dep. Politic\"]\n","pair3 = [\"nu merită\", \"nu se merită\"]\n","pair4 = [\"Omul negru nu are o înfățișare specifică, întrucât concepțiile privind chipul unui monstru variază de la o gospodărie la alta, chiar în cadrul aceleiași comunități.\",\n","         \"Omul negru nu are o înfățișare specifica, întrucât concepțile privind chipul unui monstru variază de la o gospodărie la alta, chiar în cadrul aceleași comunități.\"]\n","pair5 = [\"Rămas-bun\", \"Rămas bun\"]\n","pair6 = [\"700 de milioane de euro\", \"700 milioane euro\"]\n","pair7 = [\"Poartă numele fizicianului englez P.A.M. Dirac  care a utilizat-o extensiv în formularea sa a mecanicii cuantice, dar prezența ei în matematică este mai veche și e de exemplu implicită în folosirea integralei Stieltjes.\",\n","         \"Poartă numele fizicianului englez P.A.M. Dirac  care a utilizato extensiv în formularea sa a mecanicii cuantice, dar prezența ei în matematică este mai veche și e de exemplu implicită în folosirea integralei Stieltjes.\"]\n","pair8 = [\"copilul a prezentat afecțiuni respiratorii\", \"copilul a acuzat afecțiuni respiratorii\"]\n","pair9 = [\"Și vă întreb pe toți\", \"Și vă întreb cu toții\"]\n","\n","tokenized_pair = roberta_tokenize(pair1)\n","\n","print(tokenized_pair[0][0])\n","print(tokenized_pair[0][1])"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1661808146713,"user":{"displayName":"Nicolae Rosca","userId":"09067692646217768074"},"user_tz":-180},"id":"RZjNRr5Psb1i","outputId":"aa3e270d-e551-4754-b02d-7fe3d2a2fd4a"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'input_ids': [0, 1362, 17566, 69140, 2], 'attention_mask': [1, 1, 1, 1, 1]}\n","{'input_ids': [0, 115567, 1162, 2], 'attention_mask': [1, 1, 1, 1]}\n","[3, [('dif', 1, (1362, 115567)), ('dif', 2, (17566, 1162)), ('del', 3, 69140)]]\n","['▁că', 'ști', 'guri']\n","['▁câștig', 'uri']\n","[3, [('dif', 0, ('▁că', '▁câștig')), ('dif', 1, ('ști', 'uri')), ('del', 2, 'guri')]]\n"]}],"source":["import _pickle\n","\n","def print_matrix(d):\n","  for row in d:\n","    print(row)\n","\n","def findMin(d, i, s):\n","  min = d\n","  if i[0] < min[0]:\n","     min = i\n","  if s[0] < min[0]:\n","     min = s\n","  return min  \n","\n","def diff_finder(source, target):\n","  \n","  lenS = len(source) + 1\n","  lenT = len(target) + 1\n","\n","  d = [[[0, []] for x in range(lenT)] for y in range(lenS)] \n","  for i in range(1, lenS):\n","    d[i][0][0] = i\n","    for k in range(i):\n","      d[i][0][1].append(('del', k, source[k]))\n","\n","  for j in range(1, lenT):\n","    d[0][j][0] = j\n","    for k in range(j):\n","      d[0][j][1].append(('add', k, target[k]))\n","\n","  for i in range(1, lenS):\n","    for j in range(1, lenT):\n","      if source[i - 1] == target[j - 1]:\n","        subCost = 0\n","      else:\n","        subCost = 1\n","      \n","      insert = _pickle.loads(_pickle.dumps(d[i][j - 1], -1)) #= 'add'\n","\n","      insert[0] += 1\n","      insert[1].append(('add', i , target[j - 1]))\n","      \n","      delete = _pickle.loads(_pickle.dumps(d[i - 1][j], -1)) #= 'add'\n","      delete[0] += 1\n","      delete[1].append(('del', i - 1,  source[i - 1]))\n","\n","      subst = _pickle.loads(_pickle.dumps(d[i-1][j-1], -1))\n","\n","      if(subCost != 0):\n","        subst[0] += 1\n","        subst[1].append(('dif', i - 1 , (source[i - 1], target[j - 1])))\n","\n","      min = findMin(delete, insert, subst)\n","      d[i][j] = min\n","  #print_matrix(d)\n","  return d[lenS - 1][lenT - 1]\n","\n","#source = tokenized_pair[0][1]\n","#target = tokenized_pair[0][0]\n","\n","source = tokenizer(pair1[1])\n","target = tokenizer(pair1[0])\n","print(source)\n","print(target)\n","print(diff_finder(source['input_ids'], target['input_ids']))\n","\n","source = tokenized_pair[0][1]\n","target = tokenized_pair[0][0]\n","print(source)\n","print(target)\n","print(diff_finder(source, target))"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":346,"status":"ok","timestamp":1661810921304,"user":{"displayName":"Nicolae Rosca","userId":"09067692646217768074"},"user_tz":-180},"id":"odlgwJoRjppK"},"outputs":[],"source":["import json\n","\n","def correction_masks(wrong_token, diff):\n","  buckets_correction = [0] * len(wrong_token)\n","  buckets_add = [0] * (len(wrong_token) * 2)\n","  for correction in diff[1]:\n","    if correction[0] != 'add': #dif = 1, del = 2\n","      if correction[0] == 'dif':\n","        buckets_correction[correction[1]] = 1\n","      if correction[0] == 'del':\n","        buckets_correction[correction[1]] = 2\n","    else:\n","      buckets_add[correction[1]] += 1\n","\n","  return [buckets_correction, buckets_add[0:len(wrong_token)]]\n","\n","\n","def count_corrections(right, wrong, file_new):\n","\n","  right_token = tokenizer(right, truncation=True)\n","  wrong_token = tokenizer(wrong, truncation=True)\n","\n","  diff = diff_finder(wrong_token['input_ids'], right_token['input_ids'])\n","  [correction_mask, add_mask] = correction_masks(wrong_token['input_ids'], diff)\n","  json_dump = {'input_ids':wrong_token['input_ids'], \n","               'attention_mask' :wrong_token['attention_mask'], \n","               'labels':correction_mask, 'add_mask':add_mask}\n","\n","  file_new.write(json.dumps(json_dump))\n","  file_new.write('\\n')\n","\n","  return diff\n","\n","def find_consecutive_adds(corections):\n","  consecutive_adds_count = 0\n","  consecutive_adds_idex = None\n","  max_count = 0\n","  if(corections[0] != 0):\n","    for correction in corections[1]:\n","      if correction[0] == 'add':\n","        if correction[1] != consecutive_adds_idex:\n","          consecutive_adds_count = 1\n","          consecutive_adds_idex = correction[1]\n","        else:\n","          consecutive_adds_count += 1\n","      else:\n","        max_count = consecutive_adds_count\n","        consecutive_adds_count = 0\n","        consecutive_adds_idex = None\n","  if consecutive_adds_count > max_count:\n","    max_count = consecutive_adds_count\n","  return max_count\n","\n","\n","def create_dataset(filename_old_dataset, filename_new_dataset, top_k):\n","  file_old = open(filename_old_dataset, \"r\")\n","  file_new = open(filename_new_dataset, \"w\")\n","  count = 0\n","  \n","  while True:\n","    count += 2\n","    if count > 2*top_k:\n","      break\n","    line_correct = file_old.readline()\n","    line_wrong = file_old.readline()\n","    if (not line_correct) or (not line_wrong):\n","      break\n","    diff = count_corrections(line_correct, line_wrong, file_new)\n","    max_adds_curr = find_consecutive_adds(diff)\n","\n","  file_old.close()\n","  return"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":6957797,"status":"ok","timestamp":1661817881650,"user":{"displayName":"Nicolae Rosca","userId":"09067692646217768074"},"user_tz":-180},"id":"AJMUAIBVqMkR"},"outputs":[],"source":["original_dataset = 'drive/MyDrive/Colab Notebooks/corpus/10_mil_dirty_clean_better.txt'\n","new_dataset = 'drive/MyDrive/Colab Notebooks/corpus/new_dataset.txt'\n","\n","file_new = open(new_dataset, \"w\")\n","file_new.write('')\n","file_new.close()\n","\n","create_dataset(original_dataset, new_dataset, 100000)"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":102513,"status":"ok","timestamp":1661819053094,"user":{"displayName":"Nicolae Rosca","userId":"09067692646217768074"},"user_tz":-180},"id":"K3JJEGTJptnf"},"outputs":[],"source":["original_dataset = 'drive/MyDrive/Colab Notebooks/corpus/train.txt'\n","new_dataset = 'drive/MyDrive/Colab Notebooks/corpus/validation.txt'\n","\n","file_new = open(new_dataset, \"w\")\n","file_new.write('')\n","file_new.close()\n","\n","create_dataset(original_dataset, new_dataset, 10000)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VXj_lp8KKWdn"},"outputs":[],"source":["original_dataset = 'drive/MyDrive/Colab Notebooks/corpus/10_mil_dirty_clean_better.txt'\n","new_dataset = 'drive/MyDrive/Colab Notebooks/corpus/big_dataset.txt'\n","\n","file_new = open(new_dataset, \"w\")\n","file_new.write('')\n","file_new.close()\n","\n","create_dataset(original_dataset, new_dataset, 1000000)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNdW/uNMgcuTcHCfIuSMAsA","collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1OgoIZFgexVYx95lHFbM1XGZG2ZG6QuoN","name":"TokenTagging.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.10.6 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"vscode":{"interpreter":{"hash":"f1d568339fc5b4ca6a92cf643bc31ec6175ed971152279f5dff2ba8815adcbe2"}}},"nbformat":4,"nbformat_minor":0}
