{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, RobertaForMaskedLM, RobertaForTokenClassification, MT5ForConditionalGeneration\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base', model_max_length=128)\n",
    "custom_tokens = [f\"<extra_id_{i}>\" for i in range(100)]\n",
    "tokenizer.add_tokens(custom_tokens)\n",
    "\n",
    "model_masked = RobertaForMaskedLM.from_pretrained('xlm-roberta-large')\n",
    "\n",
    "mt5_model_directory = \"models/mt5-base-finetuned-unmask/checkpoint-28000\"\n",
    "mt5_model= MT5ForConditionalGeneration.from_pretrained(mt5_model_directory)\n",
    "mt5_tokenizer = AutoTokenizer.from_pretrained(mt5_model_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n"
     ]
    }
   ],
   "source": [
    "add_model= RobertaForTokenClassification.from_pretrained(\"models/add-roberta\", local_files_only=True)\n",
    "mod_model= RobertaForTokenClassification.from_pretrained(\"models/delete-modify-roberta\", local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n"
     ]
    }
   ],
   "source": [
    "add_model= RobertaForTokenClassification.from_pretrained(\"models/add-roberta-100k\", local_files_only=True)\n",
    "mod_model= RobertaForTokenClassification.from_pretrained(\"models/delete-modify-roberta-100k\", local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<extra_id_0>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(250002))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 48721, 405, 10965, 32, 7140, 36, 409, 115, 318, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [0, 48721, 10965, 370, 32, 7140, 36, 409, 26223, 318, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(\"Salut ce faci? Cum o mai duci\"))\n",
    "print(tokenizer(\"Salut faci tu? Cum o mai druci\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens1 = [0, 48721, 405, 10965, 32, 7140, 36, 409, 115, 318, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens2 = [0, 48721, 10965, 370, 32, 7140, 36, 409, 26223, 318, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0=<s> 48721=Salut 405=ce 10965=faci 32=? 7140=Cum 36=o 409=mai 115=du 318=ci 2=</s> 0=<s> 48721=Salut 10965=faci 370=tu 32=? 7140=Cum 36=o 409=mai 26223=dru 318=ci 2=</s> \n",
      "<s> Salut ce faci ? Cum o mai du ci </s> <s> Salut faci tu ? Cum o mai dru ci </s> "
     ]
    }
   ],
   "source": [
    "for token in tokens1:\n",
    "    print(str(token) + '=' + tokenizer.decode(token), end =\" \")\n",
    "\n",
    "for token in tokens2:\n",
    "    print(str(token) + '=' + tokenizer.decode(token), end =\" \")\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "for token in tokens1:\n",
    "    print(tokenizer.decode(token), end =\" \")\n",
    "\n",
    "for token in tokens2:\n",
    "    print(tokenizer.decode(token), end =\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_del_mod(inputs):\n",
    "    with torch.no_grad():\n",
    "        logits = mod_model(**inputs).logits\n",
    "\n",
    "    predicted_token_class_ids = logits.argmax(-1)\n",
    "    return predicted_token_class_ids[0].numpy().tolist()\n",
    "\n",
    "def generate_add(inputs):\n",
    "    with torch.no_grad():\n",
    "        logits = add_model(**inputs).logits\n",
    "\n",
    "    predicted_token_class_ids = logits.argmax(-1)\n",
    "    return predicted_token_class_ids[0].numpy().tolist()\n",
    "\n",
    "def generate_mt5(input_text):\n",
    "  input_ids = mt5_tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cpu\")\n",
    "  output = mt5_model.generate(input_ids, max_length=200)\n",
    "  return mt5_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def procentage_similarity(Q, Mi):\n",
    "  levDis = levenshtein_distance(Q, Mi)\n",
    "  bigger = max(len(Q), len(Mi))\n",
    "  return (bigger - levDis) / bigger\n",
    "\n",
    "def combine_add(original, to_replace, nr_of_tokens):\n",
    "    for i in range(nr_of_tokens):\n",
    "        result = re.search(custom_tokens[i]+'(.*)'+custom_tokens[i+1], to_replace)\n",
    "        original = original.replace(custom_tokens[i], result.group(1))\n",
    "    \n",
    "    return original\n",
    "\n",
    "def correct_mod(inputs):\n",
    "    outputs = model_masked(**inputs)\n",
    "    predictions = outputs[0]\n",
    "    _, sorted_idx = predictions[0].sort(dim=-1, descending=True)\n",
    "    predicted_index = [sorted_idx[i, 0].item() for i in range(0,len(sorted_idx))]\n",
    "    return predicted_index\n",
    "\n",
    "\n",
    "def score(input_ids):\n",
    "    \"\"\"Calculate the perplexity of a batch of tokenized sentences.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        loss = model_masked(input_ids, labels=input_ids).loss\n",
    "    return torch.exp(loss).tolist()  # Perplexity is the exponential of the loss\n",
    "\n",
    "def calculate_similarity(embedding1, embedding2):\n",
    "    return cosine_similarity([embedding1], [embedding2])[0][0]\n",
    "\n",
    "def correct(text, fill_mask, mode):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    if len(inputs['input_ids'] > 128):\n",
    "        pass\n",
    "\n",
    "    out_mod = generate_del_mod(inputs)\n",
    "    out_add = generate_add(inputs)\n",
    "\n",
    "    #if 1 in out_add:\n",
    "    #    print(\"WE HAVE ONE\")\n",
    "\n",
    "    mod_i = 0\n",
    "    to_mod = inputs\n",
    "    to_mod = to_mod['input_ids'][0]\n",
    "\n",
    "    while mod_i != len(out_mod):\n",
    "        if out_mod[mod_i] == 2:\n",
    "            to_mod = torch.cat((to_mod[:mod_i], to_mod[mod_i+1:]))\n",
    "            out_mod.pop(mod_i)\n",
    "\n",
    "            if mode == 'mt5':\n",
    "                add = out_add[mod_i]\n",
    "                out_add.pop(mod_i)\n",
    "                if add != 0:\n",
    "                    #print(\"WE HAVE ONE\")\n",
    "                    #print(text)\n",
    "                    if mod_i != len(out_add):\n",
    "                        out_add[mod_i] += add\n",
    "                    else:\n",
    "                        out_add[mod_i - 1] += add\n",
    "            continue\n",
    "        mod_i += 1\n",
    "\n",
    "    print(\"OUT MOD\")\n",
    "    print(out_mod)\n",
    "\n",
    "    print(\"OUT ADD\")\n",
    "    print(out_add)\n",
    "\n",
    "    print(len(out_mod))\n",
    "    print(len(to_mod))\n",
    "\n",
    "\n",
    "    mod_i = 0\n",
    "\n",
    "    while mod_i != len(out_mod):\n",
    "\n",
    "        if out_mod[mod_i] == 1:\n",
    "            t2 = torch.tensor([250001])\n",
    "            to_mod = torch.cat((to_mod[:mod_i], t2, to_mod[mod_i+1:]))\n",
    "\n",
    "        mod_i += 1\n",
    "    to_mod_inter = inputs\n",
    "    to_mod_inter['input_ids'] = to_mod.unsqueeze(0)\n",
    "    to_mod_inter['attention_mask'] = torch.ones(to_mod.size()).unsqueeze(0)\n",
    "    res = correct_mod(to_mod_inter)\n",
    "    if mode == 'mt5':\n",
    "        appearences = 0\n",
    "        for i in range(len(out_add)):\n",
    "            if out_add[i] == 1:\n",
    "                extra = 250002 + appearences\n",
    "                res.insert(i, extra)\n",
    "                appearences +=1\n",
    "        to_mt5 = tokenizer.decode(res[1:-1])\n",
    "        print(to_mt5)\n",
    "        out_mt5 = generate_mt5(to_mt5)\n",
    "        to_mt5 = combine_add(to_mt5, out_mt5, appearences)\n",
    "        to_mt5 = to_mt5.strip() \n",
    "        return to_mt5 + '\\n'\n",
    "    else:\n",
    "         return tokenizer.decode(res[1:-1]) + '\\n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'correct' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mMrg pe jos\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mprint\u001b[39m(correct(text, \u001b[39m\"\u001b[39m\u001b[39mbase\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmt5\u001b[39m\u001b[39m\"\u001b[39m)) \u001b[39m#default, mt5\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'correct' is not defined"
     ]
    }
   ],
   "source": [
    "text = \"Mrg pe jos\"\n",
    "print(correct(text, \"base\", \"mt5\")) #default, mt5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m i \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[39mwhile\u001b[39;00m i \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(content):\n\u001b[0;32m----> 6\u001b[0m     res \u001b[39m=\u001b[39m correct(content[i], \u001b[39m\"\u001b[39;49m\u001b[39mbase\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mmt5\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      7\u001b[0m     i \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[14], line 113\u001b[0m, in \u001b[0;36mcorrect\u001b[0;34m(text, fill_mask, mode)\u001b[0m\n\u001b[1;32m    111\u001b[0m to_mod_inter[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m to_mod\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m    112\u001b[0m to_mod_inter[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones(to_mod\u001b[39m.\u001b[39msize())\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[0;32m--> 113\u001b[0m res \u001b[39m=\u001b[39m correct_mod(to_mod_inter)\n\u001b[1;32m    114\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmt5\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    115\u001b[0m     appearences \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "Cell \u001b[0;32mIn[14], line 44\u001b[0m, in \u001b[0;36mcorrect_mod\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m     42\u001b[0m outputs \u001b[39m=\u001b[39m model_masked(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\n\u001b[1;32m     43\u001b[0m predictions \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m---> 44\u001b[0m sorted_preds, sorted_idx \u001b[39m=\u001b[39m predictions[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49msort(dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, descending\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     45\u001b[0m predicted_index \u001b[39m=\u001b[39m [sorted_idx[i, \u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mitem() \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m,\u001b[39mlen\u001b[39m(sorted_idx))]\n\u001b[1;32m     46\u001b[0m \u001b[39mreturn\u001b[39;00m predicted_index\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#SAFE TO USE\n",
    "with open(\"corpus/results_2/golden_corpus_128_filtered_well_formed_no_duplicates_original.txt\", \"r\", encoding=\"utf-8\") as infile:\n",
    "    content = infile.readlines()\n",
    "    i = 0\n",
    "    while i < len(content):\n",
    "        res = correct(content[i], \"base\", \"mt5\")\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'group'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m i \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[39mwhile\u001b[39;00m i \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(content):\n\u001b[1;32m     11\u001b[0m     \u001b[39m#res = correct(content[i], \"base\", \"mt5\")\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     res \u001b[39m=\u001b[39m correct(content[i], \u001b[39m\"\u001b[39;49m\u001b[39mbase\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mmt5\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     13\u001b[0m     outfile\u001b[39m.\u001b[39mwrite(res)\n\u001b[1;32m     14\u001b[0m     i \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[6], line 124\u001b[0m, in \u001b[0;36mcorrect\u001b[0;34m(text, fill_mask, mode)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[39m#print(to_mt5)\u001b[39;00m\n\u001b[1;32m    123\u001b[0m out_mt5 \u001b[39m=\u001b[39m generate_mt5(to_mt5)\n\u001b[0;32m--> 124\u001b[0m to_mt5 \u001b[39m=\u001b[39m combine_add(to_mt5, out_mt5, appearences)\n\u001b[1;32m    125\u001b[0m to_mt5 \u001b[39m=\u001b[39m to_mt5\u001b[39m.\u001b[39mstrip() \n\u001b[1;32m    126\u001b[0m \u001b[39mreturn\u001b[39;00m to_mt5 \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[6], line 29\u001b[0m, in \u001b[0;36mcombine_add\u001b[0;34m(original, to_replace, nr_of_tokens)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(nr_of_tokens):\n\u001b[1;32m     28\u001b[0m     result \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msearch(custom_tokens[i]\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m(.*)\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39mcustom_tokens[i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m], to_replace)\n\u001b[0;32m---> 29\u001b[0m     original \u001b[39m=\u001b[39m original\u001b[39m.\u001b[39mreplace(custom_tokens[i], result\u001b[39m.\u001b[39;49mgroup(\u001b[39m1\u001b[39m))\n\u001b[1;32m     31\u001b[0m \u001b[39mreturn\u001b[39;00m original\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'"
     ]
    }
   ],
   "source": [
    "#WILL OVERWRITE\n",
    "#file_in = \"corpus/results_2/W-sentence-original.txt\"\n",
    "#file_out = \"corpus/results_2/W-sentence-prediction-10M-base.txt\"\n",
    "file_in = \"corpus/results_2/NAC-sentences-original.txt\"\n",
    "file_out = \"corpus/results_2/NAC-sentences-prediction-100k-mt5.txt\"\n",
    "with open(file_in, \"r\", encoding=\"utf-8\") as infile:\n",
    "    content = infile.readlines()\n",
    "    with open(file_out, \"w\", encoding=\"utf-8\") as outfile:\n",
    "        i = 0\n",
    "        while i < len(content):\n",
    "            #res = correct(content[i], \"base\", \"mt5\")\n",
    "            res = correct(content[i], \"base\", \"mt5\")\n",
    "            outfile.write(res)\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(filename_old_dataset, filename_new_dataset):\n",
    "  file_old = open(filename_old_dataset, \"r\")\n",
    "  file_new = open(filename_new_dataset, \"w\")\n",
    "  \n",
    "  while True:\n",
    "    line = file_old.readline()\n",
    "    if (not line):\n",
    "      break\n",
    "    \n",
    "    encoding = tokenizer(line)\n",
    "    json_encoding = json.dumps(encoding.data)\n",
    "    file_new.write(json_encoding + '\\n')\n",
    "\n",
    "  file_old.close()\n",
    "  file_new.close()\n",
    "  return\n",
    "\n",
    "original_dataset = 'corpus/results_2/golden_corpus_128_filtered_well_formed_no_duplicates.txt'\n",
    "new_dataset = 'corpus/results_2/golden_corpus_128_filtered_well_formed_no_duplicates_inter.txt'\n",
    "\n",
    "file_new = open(new_dataset, \"w\")\n",
    "file_new.write('')\n",
    "file_new.close()\n",
    "\n",
    "tokenize_dataset(original_dataset, new_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
